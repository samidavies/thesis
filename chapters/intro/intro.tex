\chapter{Introduction and Background}

Many optimization problems can be modeled as linear programs of the form $\max_x\{c^T x | Ax \leq b\}$.
There are methods to solve linear programs efficiently and obtain a fractional solution, 
but often any valuable $x$ is integral. 
Finding optimal integral solutions is generally theoretically intractable, 
so instead we often take a fractional solution and devise some algorithm that \emph{rounds} 
the fractional solution to an integral one. 
The rounding algorithm is seen as decent if not too much is lost in the objective value from moving from the fractional solution to the integral one.


In designing linear programming relaxations, we define that \emph{integrality gap} of an LP
to be the maximum ratio over all input instances $I$ between the optimal objective value of the integral solution of $I$ and the optimal objective value of 
the relaxation of $I$.
The integrality gap is the standard way of measuring 
how well a linear program actually captures the problem. 


We accept that it is extremely likely that there exists a separation 
between the set of ``easy'' and the set of ``hard'' problems, i.e. those for which we can always find an optimal solution in time polynomial in the 
size of the problem, and those for which we cannot. Skipping over the complexity formalities that can be found in many textbooks, the latter class is referred to as
the set of \emph{\textbf{NP}-hard} problems.
Instead, we work towards obtaining approximate solutions efficiently. 
An \emph{$\alpha$-approximation algorithm} for an optimization problem is
an algorithm which given any input instance to the problem is guaranteed to return a solution whose value is
within an $\alpha$ factor 
of the value of the optimal solution, and further  the algorithm runs in time polynomial in the instance's size\footnote{In some of the literature, 
the runtime is not specified in the definition, but in this thesis everything will be in polynomial time, so we build it into the definition.}.

Designing LPs with small integrality gap---where small depends on the target approximation factor---can be difficult. 
One normally begins by writing down the basic constraints that arise from the problem statement; let us call this the basic LP for now. 
To judge the quality of the basic LP, the next step is to try and think of instances that might lead to a larger-than-desired integrality gap.
If indeed there is an instance certifying that the integrality gap is large, then we have to come up 
with additional constraints to add the LP that will eliminate the bad instances. 
Sometimes this ad hoc approach works relatively well, in that coming up with constraints to eliminate bad instances 
is not too difficult given previous work, intuition, or luck\footnote{In the Santa Claus problem, this ad hoc approach works well enough for our purposes!}.

But other times, this cycle of dreaming up new integrality gap instances and then nixing those instances with new constraints can go for a while.
% \begin{itemize}
% \item(a) enters a vicious cycle of thinking up new integrality gap instances, then adding new constraints to get rid of the instances, 
% then thinking up more integrality gap instances, and so on and so forth until you die, or
% \item(b) (potentially after repeitions of (a)) is eventually unable to come up with integrality gap instances, 
% but also cannot do anything useful in the rounding or analysis. 
% \end{itemize}
Even if there is an LP for which there are no obvious integrality gap instances, 
it might difficult to come up with a way to round an LP solution or analyze a rounding procedure.

Appealing though these options may be, hierarchies can help avoid them. 
LP (or SDP) hierarchies provide a systematic way to add new constraints and variables to an LP (or SDP) in order to strengthen them.
Additionally, there are nice properties of solutions to lifted LPs and SDPs that make analyzing rounding procedures simpler.
Sometimes after analyzing the problem using a lifted solution, it is possible to reverse-engineer a simpler LP---one that is not necessarily a lifted LP---
by only using the constraints and variables enforced by the lift that are needed in the analysis of the rounding procedure
\footnote{This is what happened in our work on scheduling with 
communication delays on identical machines. One can view the Sparsest Cut algorithm by Arora, Rao, and Vazirani this way, too.}. 
Other times, the analysis uses properties of the lifted solution that are global in the sense that it is not obvious how to extract a compact LP\footnote{This is what happened in our work in scheduling with 
communication delays on related machines.}. 

For the purposes of this thesis, we will only give some background on the Sherali-Adams hierarchy, but the 
interested reader should see the chapter by Chlamtac and Tulsiani in the first reference or the work by Laurent for more details\cite{SAref, Comparison-of-Hierarchies-Laurent-MOR03}.
From the basic LP relaxation, we strengthen as specified by the \emph{rank} of the lift, where higher lifts are stronger.
As one should expect, the increased strength with higher rank comes with higher complexity;
an integer program on $n$ variables, $x_1,\ldots,x_n$, lifted to rank $r$ can be solved in time $n^{O(r)}$.
At a high level, a Sherali-Adams lift of rank $r$ enforces a consistent probability distribution on sets of variables 
of size at most $r$. 
More specifically, observe that for $K$ a polytope of a relaxation of a 0/1 integer program on $n$ variables, 
we can view any point in conv$(K \cap \{0,1\}^n)$ 
as a probability distribution $X$ over points in the solution set $K \cap \{0,1\}^n$.
For $I \subseteq [n]$ with $|I| \leq r$, one could manually add variables $y_I$ representing the probability over the distribution $X$ 
that $X_i=1$ for all $i \in I$ and constraints enforcing these probabilities agree according to inclusion-exclusion.
We would ideally like to enforce that $y_I = \prod_{i \in I} y_i$; instead the Sherali-Adams lift requires the following:
\begin{definition}
Let $\pazocal{P}_r([n]) = \{I \subseteq [n] \mid |I| \leq r\}$ and $K=\{x \in \mathbb{R}^n \mid Ax \geq b\}$ for $A \in \mathbb{R}^{m \times n}$. 
Then $SA_r(K)$, the Sherali-Adams lift of $K$ of rank $r$, is the set of vectors $y \in [0,1]^{n}$ with $y_{\emptyset}=1$ and for all $I,J \subseteq [n]$ with $|I|+|J|\leq r$,
$$
\sum_{ J' \subseteq J} (-1)^{|J'|} \cdot  \left ( \sum_{i=1}^n A_{\ell,i}y_{I \cup J' \cup \{i\}-b_{\ell}y_{I \cup J'}}\right ) \geq 0 \qquad \forall \ell \in [m].
$$
\end{definition}
More properties of the Sherali-Adams hierarchy are discussed in Chapters~\ref{chapter: S1} and \ref{chapter: S2}.


% A (rambly, yet) brief sidebar follows.
Problems for which we use approximation algorithms and that are considered in theoretical computer science 
are often motivated from practical situations. 
But it is worth noting that in the definition of an $\alpha$-approximation algorithm, 
we require that the algorithm obtains a solution whose value is only $\alpha$ off for \emph{any} input.
Depending on your persuasion, this may seem like a very strong definition, and maybe impractical... 
perhaps the inputs that make algorithm design difficult are contrived or never really arise in practice. 
So we have an---at first glance---strange juxtaposition, where we motivate a problem by saying that it is arises in practice,
but we measure success with a potentially impractical metric.
From my perspective, this view is orthogonal to the work I did during my Ph.D., as well as a lot of the most important work in TCS
\footnote{Though there is a lot of meaningful work in approximation algorithms, TCS, and beyond that 
takes runtime, memory, input characteristics, and other practical concerns into consideration, and I very well might work in this space someday.}.
The design and analysis of approximation algorithms represented in this thesis is all about the math.
Specifically, it is about discovering what mathematical structures we can superimpose onto a hard problem in order to make a little more sense 
of why it is indeed hard. 
We focus on two specific \textbf{NP}-hard problems in this thesis: Santa Claus, also known as Restricted Max-Min Fair Allocation, and scheduling 
with precedence constraints and communication delays.





\bigskip



\section{The Santa Claus problem}

In the \emph{Santa Claus problem}, the objective is to allocate gifts, $M$, to children, $J$, in such a way that the minimum value of any child is maximized. 
We assume that child $i \in M$ has value $p_{i,j}$ for gift $j \in J$ and that a child's value is the sum of the values of the gifts they receive. 
Formally, we seek an assignment $\phi: J \rightarrow M$ such that $\min_{i \in M} \sum_{\phi^{-1}(i)} p_{i,j}$ is maximized. 
Let $m$ denote the number of children and $n$ the number of gifts. 
In Santa Claus, also known as restricted Max-Min Fair Allocation, we assume that $p_{i,j} \in \{0,p_j\}$, i.e if two children both want to receive a gift, then they both value it equally.
We will refer to the unrestricted version of MMFA occasionally for background, 
where in the unrestricted version $p_{i,j}$ does not necessarily belong to $\{0,p_j\}$\footnote{In Max-Min Fair Allocation,
 we normally use the terms ``agents'' and ``items'' instead of ``children'' and ``gifts'', 
 but since we only briefly discuss the unrestricted version, we will stick with the latter lingo.}.

\subsection{Previous work}


It is \textbf{NP}-hard to approximate Santa Claus within 
a factor less than 2 \cite{BezakovaD05}. 
Though interestingly, the known approximation factors for upper bounds wildly differ between the 
restricted and unrestricted versions. 
Before our work, constructive methods for both the restricted and unrestricted versions of 
Max-Min Fair Allocation in some way use the existence of a solution to an exponentially large
\textit{configuration linear program}  \cite{SantaClaus-BansalSviridenko-STOC2006, AlgoForSantaClaus-AnnamalaiKalaitzisSvenssonSODA15, AS10, MaxMinFairAllocation-ChakrabartyChuzhoyKhannaFOCS09, ChengM18}.
 
%The CLP is a strong relaxation with an exponential number of variables, but it perhaps does not invoke much intuition,
%as it is brute forcing which bundles of gifts satisfy children. 
For a CLP with objective value $T$, 
the variables in the CLP take the form $x_{i,C}$, 
which represents whether child $i$ receives the subset of gifts $C \in  \pazocal{C}(i,T)$, 
those sets of gifts bringing child $i$ at least value $T$:
\begin{align*}(CLP) \qquad
\sum_{C \in \pazocal{C}(i,T)} x_{i,C} &\geq 1  \qquad \forall i \in M \\
\sum_{C: j \in C} \sum_{i} x_{i,C} &\leq 1 \qquad \forall j \in J \\ 
x &\geq 0.
\end{align*}

The CLP was introduced by Bansal and Sviridenko when studying the Santa Claus problem. 
They use it to give an $O(\log \log m / \log \log \log m)$- approximation for the Santa Claus problem  \cite{SantaClaus-BansalSviridenko-STOC2006}. 
Since then, progress has been made towards better constant factor approximations for the Santa Claus problem, 
with Feige proving an (unspecified constant) $O(1)$-approximation and Annamalai, Kalaitzis, and Svensson proving 
a 12.3-approximation \cite{ConstantIntegralityGapSantaClaus-Feige-SODA2008, AlgoForSantaClaus-AnnamalaiKalaitzisSvenssonSODA15}.
The CLP for the Santa Claus problem has an integrality gap of $3.808$ \cite{CM19}. 
In contrast, the CLP for unrestricted Max-Min Fair Allocation has an integrality gap of $\Omega(\sqrt{m})$, but
% Asadpour and Saberi give an $O(\log^3 m \sqrt{m})$-approximation through a beautiful randomized algorithm. 
Chakrabarty, Chuzhoy, and Khanna cleverly beat the $\sqrt{m}$ gap to obtain an $\tilde{O}(n^{\varepsilon})$-approximation 
in $n^{1/\varepsilon}$ time for $\varepsilon =\Omega \left (  \log \log n / \log n \right )$
by iteratively using solutions to the LP, 
whose gaps become smaller in each iteration\footnote{
Note that some results are in terms of $m$, and some in terms of $n$. Though in general
we think of $n$ as much larger than $m$, and in fact, 
the results of Chakrabarty, Chuzhoy, and Khanna hold replacing $n$ with $m$ for constant $\varepsilon.$
}. 
From these works, it is helpful to note that 
the algorithms rely on assigning gifts with large versus small $p_{i,j}$
differently \cite{AlgoForSantaClaus-AnnamalaiKalaitzisSvenssonSODA15,MaxMinFairAllocation-ChakrabartyChuzhoyKhannaFOCS09}. 
Similar ideas have been used in scheduling to minimize makespan \cite{MakespanScheduling-Svensson-STOC11, SchedulingUnrelatedParallelMachines-LenstraShmoysTardos-FOCS87}.


More generally, studying Max-Min Fair Allocation can be viewed from the lens of investigating sufficient conditions for 
perfect matchings in \emph{bipartite hypergraphs}. 
A hypergraph $\pazocal{H} = (V \dot\cup W, \pazocal{E})$ is called bipartite if for all
$e \in \pazocal{E}$, $|e \cap V|=1$, and hyperedges $F \subseteq \pazocal{E}$ form a (left) perfect matching 
if they are disjoint and every node in $V$ is contained in exactly one edge in $F$.
In particular, a solution to the CLP is a fractional perfect matching on the hypergraph 
$ \left(M \dot\cup J, \bigcup_{i \in M}\pazocal{C}(i,T) \right)$.  
Finding perfect matchings in bipartite hypergraphs is in general \textbf{NP}-hard, though
\emph{Haxell's condition} defines a sufficient condition:
\begin{theorem} \cite{HypergraphMatchingsHaxell95}
Let $\pazocal{H} = (V \dot\cup W, \pazocal{E})$ be a bipartite hypergraph with $|e| \leq r$ for all $e \in \pazocal{E}$.
Then either $\pazocal{H}$ contains a (left) perfect matching or there are subsets $V' \subseteq V$ and $W' \subseteq W$
so that all hyperedges incident to $V'$ intersect $W'$ and $|W'| \leq (2r-3)(|V'|-1)$.
\end{theorem}
While the proof of Haxell's condition relies on a potentially exponential time argument, 
Annamalai made the argument polynomial by restricting edge sizes to be uniform and allowing some slack in the inequality \cite{FindingPerfectMatchingsInHypergraphs-Annamalai-SODA16}.
His argument, in addition to the one made in the 12.33-approximation algorithm by Annamalai, Kalaitzis, and Svensson,
takes Haxell's argument and makes it polynomial by introducing an augmenting tree. 
This generalizes the notion of an augmenting path used to find maximum matchings in bipartite graphs.
Instead of swapping every other edge in an augmenting path, as is the case for a bipartite graph,
the augmenting tree swaps sets of alternating hyperedges to find more space in the hypergraph.








% Given a set of agents and a set of goods, a natural question 
% is how does one distribute the goods in a way that is fair? 
% While the notion of algorithmic fairness is now a rich area of theoretical computer science, 
%  Max-Min Fair Allocation presents a simple notion of fairness that leads to a practical combinatorial optimization problem.
%  In \emph{Restricted Max-Min Fair Allocation},
% the objective is to allocate items to agents in such a way that the minimum value of any agent is maximized.
% We assume that agent $a \in A$ has value $p_{a,i}$ for item $i \in I$, where $p_{a,i} \in \{0,p_i\}$ for $p_i \in \mathbb{Z}^+$, and that an agent's value is the sum of the values of the items they receive.
% Formally, we seek an assignment $\phi :I \rightarrow A $ such that $\min_{a \in A} \sum_{i \in \phi^{-1}(a)}p_{a,i}$ is maximized. 

% Let $m$ denote the number of agents and $n$ the number of items.
% This problem is affectionately known as the Santa Claus problem, 
% with lingo swapping the terms ``agents'' and ``items'' for ``children'' and ``presents''.
% More generally, 
% Restricted Max-Min Fair Allocation can be viewed
% %  from the lens of investigating
%  as a problem of finding sufficient conditions for 
% perfect matchings in bipartite hypergraphs. 
% A hypergraph $\mathcal{H} = (V \dot\cup W, \mathcal{E})$ is called bipartite if for all
% $e \in \mathcal{E}$, $|e \cap V|=1$, and hyperedges $F \subset \mathcal{E}$ form a (left) perfect matching 
% if they are disjoint and every node in $V$ is contained in exactly one edge in $F$.
% In particular, a solution to the configuration linear program is a fractional perfect matching on the hypergraph 
% $ \left(A \dot\cup I, \bigcup_{a \in A}\mathcal{C}(a,T) \right)$.  
% While finding perfect matchings in bipartite hypergraphs is in general NP-hard, 
% in some classes of hypergraphs they can be found efficiently~\cite{AKS17, Annamalai15}.
% Such algorithms generalize the notion of an augmenting path used to find a maximum matchings in bipartite graphs.


\subsection{Our contribution}
In my work on the Santa Claus problem with Thomas Rothvoss and Yihao Zhang,
we exploit an underlying matroid to design a clean bipartite hypergraph matching algorithm \cite{DaviesRZ20}. 
This leads to an improved approximation factor (from 12.3 to $4+ \varepsilon$) compared to the solution of a new linear program relaxation\footnote{After obtaining our results, 
we learned that Cheng and Mao simultaneously and independently obtained 
a $6 + \varepsilon$ approximation to the Santa Claus problem
by altering the algorithm by Annamalai, Kalaitzis and Svensson \cite{ChengM18}.  
Using ideas established by Asadpour, Feige, and Saberi, they further improved this
to a $4 + \varepsilon$ approximation \cite{CM19, SantaClaus-AsadpourFeigeSaberi-APPROX2008}.}.

We build off the idea of separating gifts by their values by designating a gift $j$ as either large or small, depending on its value $p_j$. 
Our linear program has only $O(n^2)$ many variables and constraints and uses this distinction between large and small gifts. 
In particular, we introduce an expansion constraint, 
forcing children who do not entirely receive a large gift to fractionally receive at least $T$ small gifts, 
for $T$ the optimal objective value. Let
$J_L := \{ j \in J \mid p_j > \delta_2 T\}$ denote the large gifts
and $J_S := \{ j \in J \mid p_j \leq \delta_1 T\}$ denote the small gifts
for constants $0 < \delta_1 \leq \delta_2 < 1$ such that
% For fixed parameters $1 \geq \delta_2 \geq 1/4 > \delta_1 \geq 0$, where 
all gifts have values in $[0,\delta_1  T] \cup (\delta_2  T,T]$. 
$M_j$ denotes the set of children who wish to receive gift $j$, in other words the set of $i \in M$ such that $p_{i,j} > 0$.
Let $P(T,\delta_1,\delta_2)$ be the set of vectors $z \in \mathbb{R}_{\geq 0}^{J \times M}$ satisfying
\begin{eqnarray*}\label{eq:CompactLPforSC} \qquad
        \sum\limits_{j \in J_S: i \in M_j} p_j z_{ij} &\geq& T \cdot \Big( 1-\sum_{j \in J_L: i \in M_j} z_{ij}\Big)  \qquad \forall i \in M \\
        \sum\limits_{i \in M_j}z_{ij} &\leq& 1 \hspace{4.0cm} \forall j \in J \\
        z_{ij} &\leq& 1-\sum_{j' \in J_L: i \in M_{j'}} z_{ij'} \hspace{1.5cm} \forall j \in J_S \; \forall i \in M_j.
\end{eqnarray*}


The separation between large and small gifts was critical for our approach as it allowed us to define an underlying matroid.
Recall a matroid is a structure generalizing the notion of linear independence in vector spaces,
and in particular their independent sets have useful exchange properties. 
Formally, $\pazocal{M} = (X,\pazocal{I})$ is a matroid with groundset $X$ and 
independent sets $\pazocal{I} \subseteq 2^X$ if and only if the following hold:
\begin{itemize}
\item \emph{Non-emptyness}: $\emptyset \in \pazocal{I}$; 
\item \emph{Monotonicity}: For $Y \in \pazocal{I}$ and $Z \subseteq Y$ one has $Z \in \pazocal{I}$; 
\item \emph{Exchange property}: For all $Y,Z \in \pazocal{I}$ with $|Y| < |Z|$ there is an element $z \in Z \setminus Y$ so that $Y \cup \{ z\} \in \pazocal{I}$.
\end{itemize}
The bases of a matroid, denoted by $\pazocal{B}(\pazocal{M})$, are all inclusion-wise maximal independent sets, and
the convex hull of all bases is called the base polytope, denoted by $P_{\pazocal{B}(\pazocal{M})} := \text{conv}\{ \chi(S) \in \{0,1\}^X \mid S\text{ is basis}\}$,
where $\chi(S)$ is the characteristic vector of $S$.


Now consider a bipartite graph $G = (V \dot{\cup} W,E)$ with ground set $V$ on one side and
a set of resources $W$ on the other side; each resource $w \in W$ has
value $p_w \geq 0$. 
We define the problem \emph{Matroid Max-Min Allocation}, 
where the goal is to find a basis $S \in \pazocal{B}(\pazocal{M})$ and an 
assignment $\sigma : W \to S$ with $(\sigma(w),w) \in E$ so that $\min_{v \in S} \sum_{w \in \sigma^{-1}(v)} p_w$ 
is maximized. 
For $T \geq 0$ the target objective function value, 
we define a linear programming relaxation $Q(T)$ as the set of vectors 
$(x,y) \in \mathbb{R}_{\geq 0}^X \times \mathbb{R}_{\geq 0}^E$
satisfying the constraints:
\begin{align*}
 x &\in P_{\pazocal{B}(\pazocal{M})} &\\
 \sum_{w \in N(v)} p_wy_{vw} &\geq T \cdot x_v   &\forall v \in V\\
  \quad  y(\delta(w)) &\leq 1  &\forall w \in W\\
   y_{vw} &\leq x_v &\forall (v,w) \in E.
\end{align*} 

Here, the decision variable $x_v$ expresses whether element $v$ should be part of the basis, and
$y_{vw}$ expresses whether resource $w$ should be assigned to element $v$. We abbreviate 
$N(v)$ for the neighborhood of $v$ and $y(\delta(w))$ for $\sum_{v : (v,w) \in E} y_{vw}$.
Our main technical result is the folllowing: 
\begin{theorem}\label{thm:MainMatroidAlgorithm_intro} \cite{DaviesRZ20}
Suppose $Q(T) \neq \emptyset$ and membership in the matroid can be tested in time polynomial in $n$.
Then for any $\varepsilon>0$ one can find
 \[
   (x,y) \in Q \left (
    \left (\frac13 - \varepsilon \right ) \cdot T - \frac13 \cdot \max_{w \in W} p_w \right )
 \] 
with both $x$ and $y$ integral in time $n^{\Theta_{\varepsilon}(1)}$, where $n := |V| + |W|$. 
\end{theorem}


Viewing the set of children and gifts as parts of a bipartite graph, 
the sets of children that can be given a large gift form a matchable set matroid, $\pazocal{M}$.
Thus children who do not receive a large gift form bases of $\pazocal{M}^*$, the co-matroid of $\pazocal{M}$.
Set $\delta_1$ so that $\delta_1 \cdot T$ is the largest gift value that is at most
$\frac14 \cdot T$, and set $\delta_2$ so that $\delta_2 \cdot T$ is the smallest gift value that is at least $\frac14 \cdot T$. 
In an instance of the Santa Claus problem where the optimal integral solution has value $T$, 
we transform a solution in $P(T,\delta_1,\delta_2)$ into a solution of $Q(T)$ with $\max_{w \in W} p_w = \delta_1 \cdot T$.

By using properties of matroids, we find an assignment of small gifts to a basis of $\pazocal{M}^*$ by
simplifying a bipartite hypergraph matching algorithm
of Annamalai, Kalaitzis, and Svensson that builds an augmenting tree \cite{AlgoForSantaClaus-AnnamalaiKalaitzisSvenssonSODA15}.
Using Theorem~\ref{thm:MainMatroidAlgorithm_intro}, each child receiving only small gifts has value at least
$\left (\frac13 - \varepsilon \right ) \cdot T - \frac13 \cdot \delta_1 \cdot T \geq \left (\frac14 - \varepsilon \right ) \cdot T$. 
The remaining children who received no small gifts get a single large gift with value at least $\delta_2 \cdot T \geq \frac14 \cdot T$,
guaranteeing every child at least $\frac14 - \varepsilon$ of the optimal value.
Interestingly, in the case where the distribution of gift values is bimodal and the gap between $\delta_1$ and $\delta_2$ is large, 
our approximation factor improves.
In the most extreme case when gift values are either 1 or $T$, 
our algorithm returns a $3 + \varepsilon$ approximation. 



% We alter an algorithm from [cite] which given a bipartite graph $G=(U,V,E)$, with $U$ the ground set of a matroid and $V$ unit-valued resources we wish to allocate to $U$, uses hypergraph matching techniques to find a basis $S$ of a matroid and a hypergraph matching $M$ with large enough edges covering $S$. Applying this more general framework to $\mathcal{M}^*$, we find a set of children which will only receive small gifts, and an assignment of small gifts to them. Properties of matroids and the the matroid polytope are essential in our construction a clean, correct argument.

% Just as in [cite], we construct an \textit{augmenting tree}. 
% This is a collection of hyperedges found layer by layer, where every other layer belongs in the current hyper matching. algorithm swaps sets of edges in the augmenting tree to find more space in the hypergraph. The edges are swapped in such a way that the set of children covered by the matching is always independent with respect to the matroid.





\section{Scheduling with Communication Delays} 


Distributing tasks onto processors occurs in many areas of computing.
Frequently, some tasks use as their inputs the outputs of other tasks. 
The cost of transferring data between processors can be expensive, 
e.g. in data center scheduling and Deep Neural Network training~\cite{coflow, zhao2015rapier}. 
This dependence and latency in scheduling is modelled with precedence constraints and communcation delays.

% The above precedence-constrained scheduling problem models the task of distributing workloads onto multiple processors or servers, which is ubiquitous in computing. This basic setting takes the dependencies between work units into account, but not the data transfer costs between machines, which is critical in applications. A precedence constraint j ≺ j′ typically implies that the input to j′ depends on the output of j. In many real-world scenarios, especially in the context of scheduling in data centers, if j and j′ are executed on different machines, then the communication delay due to transferring this output to the other machine cannot be ignored. This is an active area of research in applied data center scheduling literature, where several new abstractions have been proposed to deal with communication delays [CZM+11, GFC+12, HCG12, SZA+18, ZZC+12, ZCB+15, LYZ+16]. Another timely example is found in the parallelization of Deep Neural Network training (the ma- chines being accelerator devices such as GPUs, TPUs, or FPGAs). There, when training the network on one sample/minibatch per device in parallel, the communication costs incurred by synchronizing the weight updates in fact dominate the overall running time [NHP+19]. Taking these costs into account, it turns out that it is better to split the network onto multiple devices, forming a “model- parallel” computation pipeline [HCB+19]. In the resulting device placement problem, the optimal split crucially depends on the communication costs between dependent layers/operators.



Given a set of $n$ jobs, and a set of $m$ machines on which the jobs can be performed, 
scheduling algorithms find an assignment of jobs to machines and time slots.
The inclusion of \emph{precedence constraints} model the flow of jobs as a directed acyclic graph, where 
 if there is an edge in the DAG from $j_1$ to $j_2$, denoted $j_1 \prec j_2$, then $j_1$ must be completed before any machine can start to process $j_2$.
The inclusion of  \emph{communication delays} model the latency between dependent jobs, 
where if $j_1 \prec j_2$ and $j_2$ is performed on a different machine than $j_1$, then at least $c_{j_1,j_2}$ time 
units must pass after the completion of $j_1$ before any machine can start to process $j_2$.

One objective of interest is minimizing \emph{makespan}, which is the time that the last machine processes its final job.
In the setting where each jobs is additionally assigned a weight, $w_j$ for job $j \in J$, and $C_j$ is the time at which job $j$ is completed,
another objective is to minimize the \emph{weighted sum of completion times}, $\sum_{j \in J} w_j C_j$.
In the \emph{identical} machines setting, all machines process all jobs equally quickly.
In the \emph{related} machines setting, each machine is equipped with some speed 
$s_i$ for $i \in [m]$ and job $j$ is processed in $p_j/s_i$ time units by machine $i$.
We are interested in \emph{non-preemptive} scheduling, where a job must be processed in its entirety on a single machine in a consecutive period of time.



\subsection{Previous work}

Overall, there was very little known about scheduling with communication delays for non-constant $c$ before our work and the concurrent work of Maiti, Rajaraman, 
Stalfa, Svitkina, and Vijayaraghavan\cite{MRSSV}, despite much practical and theoretical intrigue.
In fact, obtaining constant factor approximation algorithms to the problem is in a list of the top 10 open problems in scheduling theory 
that was proposed by Schuurman and Woeginger~\cite{SW99a}.
More recently in a 2017 survey talk, Bansal described scheduling with precedence constraints and communication delays 
as ``not understood at all" and ``almost completely open"~\cite{Bansalmapsp}.

Simple arguments obtained $O(c)$-approximations for the objective of minimizing makespan~\cite{GrahamListScheduling1966, GiroudeauKMP08}.
In a \emph{list scheduling} algorithm, a heuristic orders the jobs and then jobs are assigned according to the order to the least full machine.
List scheduling already gives a $(c+2)$-approximation for the problem, and the best known for general $c$ was a $2/3(c+1)$ approximation.
While there are tighter bounds when $c=1$, little else was known for when $c$ is large, which is the parameter setting arising in practice~\cite{MunierKonig, HanenMunier73Apx}.

Without communication delays, but with precedence constraints, Graham showed that with any ordering, list-scheduling gives a $(2-1/m)$-approximation.
And this is roughly tight, as assuming a variant of the Unique Games Conjecture, in 2010 Svensson showed that it is \textbf{NP}-hard to obtain better than a $(2-\epsilon)$-approximation 
for constant $\epsilon>0$.
It should also be noted that one can instead consider the problem with \emph{duplication}; here
a job can be performed on multiple machines in order to forego the communication cost incurred for placing dependent jobs on different machines.

One hypothesis of why this problem eluded researchers for so many decades is the lack of good LP relaxations. 
To see why, let us consider the LP of Munier and K\"{o}nig, which---together with a simple rounding procedure---was used to 
obtain a $4/3$-approximation for scheduling with communication delays in the special case when $p_j=1$ for all $j \in J$ and $c=1$. 
In the LP below, we call $\Gamma^{-}(j) = {j' \in J \mid j' \prec j}$ the \emph{predecessors} of $j$ and 
$\Gamma^{+}(j) = {j' \in J \mid j \prec j'}$ the \emph{successors} of $j$. 
Further, $C_j$ is the completion time of job $j$ and $x_{j,j'}$ is the decision variable for whether jobs $j \prec j'$ are performed on the same machine.
\begin{align*}
C_{j'}+2-x_{j,j'} &\leq C_j  &\forall j' \prec j \\
\sum_{j' \in \Gamma^{-}(j)} x_{j',j}&\leq 1 &\forall j \in J \\
\sum_{j' \in \Gamma^{+}(j)} x_{j,j''}&\leq 1 &\forall j \in J \\
0&\leq C_j   &\forall j \in J \\ 
0 \leq x_{j,j'} &\leq 1  &\forall j \prec j'
\end{align*}

Now, we could try to extend the LP above to the case when $c$ is large.
Assume the time horizon is partitioned into intervals of length $c$ and the completion time variables now refer to which $c$-length interval to schedule in,
e.g. the first $c$ length interval is from time 0 to $c$ and $C_j=0$ for job $j$
if and only if job $j$ is performed by the LP in the first interval. 
Also, we slightly change the variables $y_{j,j'}$ to be the decision variable for whether jobs $j$ and $j'$ are performed on the same machine and in the same $c$-length interval.
Then we end up with the following LP:
\begin{align*}
  C_{j'}+1-y_{j,j'}&\leq C_j  &\forall j' \prec j \\
  \sum_{j' \in J } y_{j,j'}&\leq c &\forall j \in J \\
  0&\leq C_j   &\forall j \in J \\ 
  0 \leq y_{j,j'} &\leq 1  &\forall j \prec j'
\end{align*}
Unfortunately, this LP has a non-constant integrality gap; 
for a DAG with degree at most $c$, the LP can simply set schedule all jobs in the first interval, even though an integral solution will need a superconstant number of intervals.

See Chapters \ref{chapter: S1} and \ref{chapter: S2} for a comparison on our work versus that of Maiti et. al.

\subsection{Our contribution}


In joint work with Janardhan Kulkarni, Thomas Rothvoss, Jakub Tarnawski, and Yihao Zhang, we studied the problem 
of scheduling with 
precedence constraints and communication delay $c$ on identical machines. 
In the identical machine setting, all machines have the same speed and are able to process every job. 

As outlined above, it was not clear how to construct a useful LP using an ad hoc approach.
Therefore, we used a \emph{Sherali-Adams} lift.
The LP is constructed in two steps; the first step constructs a basic assignment LP and the second step uses the variables produced from lifting the basic LP. % for $P\infty \mid \Prec, p_j=1, c\textrm{-intervals} \mid C_{\max}$ in two steps. 
Consider the assignment variables
\[
  x_{j,i,s} = \begin{cases} 1 & \textrm{if }j\textrm{ is scheduled on machine }i\textrm{ in interval }I_s \\ 0 & \textrm{otherwise} \end{cases} \quad \forall j \in J, i \in [m], s \in \{ 0,\ldots,S-1\},
%  \theta_j = index of interval where j is scheduled 
\]
and let $K$ be the set of fractional solutions to 
\begin{eqnarray*}
  \sum_{i \in [m]} \sum_{s \geq 0} x_{j,i,s} &=& 1 \quad \forall j \in J \\
  \sum_{j \in J} x_{j,i,s} &\leq& c \quad \forall i \in [m] \;\; \forall s \in \{ 0,\ldots,S-1\} \\
%  \sum_{s'\leq s} \sum_{i \in [m]} x_{j_1,i,s'} &\geq& \sum_{s' \leq s} \sum_{i \in [m]} x_{j_2,i,s'} \quad \forall j_1 \prec j_2 \;\; \forall s \in \{ 0,\ldots,S-1\} \\
  0 \leq x_{j,i,s} &\leq& 1 \quad \forall j \in J, i \in [m], s \in \{ 0,\ldots,S-1\}.
\end{eqnarray*}
%The LP asks for a (fractional) schedule where for a pair $j_1 \prec j_2$, $j_2$ cannot be scheduled in an earlier interval.
%However, it does not enforce that $j_1$ and $j_2$ be performed in separate intervals if they are not scheduled on the same machine.
A lift $x \in \textsc{SA}_r(K)$ introduces variables $x_{(j_1,i_1,s_1),(j_2,i_2,s_2)}$ that are the probability that 
job $j_1$ is scheduled in interval $s_1$ on machine $i_1$ and job
$j_2$ is scheduled in interval $s_2$ on machine $i_2$. 
Variables $C_j$ and $y_{j,j'}$ are those of our previously attempted LP, and $S$ is the number of $c$-length intervals on the time horizon.
% \begin{eqnarray*}
%   y_{j_1,j_2} &=& \begin{cases} 1 & j_1\textrm{ and }j_2\textrm{ are scheduled on the same machine in the same interval} \\ 0 & \textrm{otherwise} \end{cases} \\
%    C_j &=& \textrm{index of interval where }j\textrm{ is processed}
% \end{eqnarray*}
We let $Q(r)$ denote the set of vectors $(x,y,C)$ satisfying the following:
\begin{eqnarray*}
  y_{j_1,j_2} &=& \sum_{s \in \{ 0,\ldots,S-1\}} \sum_{i \in [m]} x_{(j_1,i,s),(j_2,i,s)} \\
  C_{j_2} &\geq& C_{j_1} + (1-y_{j_1,j_2}) \quad \forall j_1 \prec j_2 \\
  C_j &\geq& 0 \quad \forall j \in J \\ 
  x &\in& SA_r(K)
\end{eqnarray*}

Note that for our purposes, it suffices to take $r=5$, so we can solve the LP in polynomial time.
Our main technical theorem is the following:
\begin{theorem}
Consider an instance with unit-length jobs $J$, a partial order $\prec$, and parameters $c,S,m \in \setN$ such that
  $Q(r)$ is feasible for $r:=5$. Then there is a randomized algorithm with expected polynomial running time that finds a
   schedule for the instance using at most $O(\log m \cdot \log c) \cdot S$ intervals.
\end{theorem}
The Sherali-Adams lift guarantees that the triangle inequality holds on the function $d(j,j') =1-y_{j,j'}$. 
Therefore the set of jobs equipped with $d$ is a \emph{semimetric space}, where jobs close with respect to the semimetric are likely to be scheduled on the same machine
 and in the same $c$-length interval.
Equipped with a semimetric, we design an algorithm that 
clusters jobs with a clustering algorithm by Calinescu, Karloff, and Rabani~\cite{DBLP:journals/siamcomp/CalinescuKR04}.

Since we are willing to pay a $O(\log c)$ factor in the approximation, 
it suffices to partition the set of jobs into band of width $\Theta(1/\log c)$ and just schedule each band, paying $c$ between scheduling each band.
For a fixed band $J^* = \{j \mid T \leq C_j \leq T + \frac{1}{\Theta( \log n)}\}$, jobs $j,j' \in J^*$ are very likely to be scheduled on the same machine and in the 
same interval according to the LP. Using this fact, we can show that 
 the probability that a node $j \in J^*$ is assigned by the CKR algorithm to a cluster different from any of its predecessors in $J^*$ is at most $1/2$.   
Therefore the downward closed subsets of each cluster can be scheduled on an unused machine in an interval of size at most $2c$.
%  takes as input the set of jobs that have completion time in a $O(c/ \log(c))$-length interval
% according to the LP solution and
% schedules a constant fraction of these jobs 
% while respecting precedence constraints~\cite{CKR01}. 
Repeating this iteratively, and inserting $c$ idle time slots between each iteration to account for the the communication delay between jobs in different iterations,
we show that with high probability this procedure schedules all the jobs\footnote{Potentially a set of jobs is left after $O(\log m)$ iterations, 
but at that point, they can be scheduled together with high probability.} after $\Theta(\log m)$ iterations. 



We can reduce from the general setting to this special unit-length jobs setting with the next theorem.
\begin{theorem}
  Given an instance with unit-length jobs, 
  suppose that there is a polynomial time algorithm that takes a solution for the LP $Q(r)$ with parameters $m,c,S \in \setN$ and $r \geq 5$
  and transforms it into a schedule using at most $\alpha \cdot S$
  intervals. Then there is a polynomial time  $O(\alpha)$-approximation for scheduling with communication delays on identical machines for arbitrary job lengths.
%	An $\alpha$-approximation for $\p \infty \mid \Prec, p_j=1, \Intervals \mid C_{\max}$ implies an $O(\alpha)$-approximation for $P \mid \Prec, c \mid C_{\max}$.
\end{theorem}
Overall, this implies a $O(\log m \cdot \log c)-$approximation algorithm for scheduling with communication delays on identical machines.


Sherali-Adams gives the insight that the following compact LP actually suffices for our analysis.
Our compact LP is the following: 
\begin{align*}
y_{j_1,j_1}&=1 \qquad \forall j_1 \in J\\
y_{j_1,j_2} &= y_{j_2,j_1} \qquad \forall j_1,j_2 \in J\\
1-y_{j_1,j_2} &\leq 1-y_{j_1,j_3} + 1-y_{j_2,j_3} \qquad \forall j_1,j_2,j_3 \in J\\
\sum_{j \in J} y_{j_1,j} &\leq c \qquad \forall j_1 \in J\\
 C_{j_2} &\geq C_{j_1} + (1-y_{j_1,j_2}) \qquad \forall j_1 \prec j_2 \\
  C_j &\geq 0 \quad \forall j \in J \\
  0 \leq y_{j,j'} &\leq 1 \qquad \forall j,j' \in J
\end{align*}

% In the objective of minimizing makespan, where the makespan is denoted $C_{\textrm{max}}$ for maximum completion time.


% The PI has worked on scheduling in the presence of precedence constraints and communication delays
%  with Janardhan Kulkarni, Thomas Rothvoss, Jakub Tarnawski, and Yihao Zhang. 
% For the base problem of $P_{\infty} \mid p_j=1, c \mid C_{\textrm{max}}$
% (minimizing makespan with unit processing times and communication delays of length $c$ on arbitrarily many machines),



% These results extend to include arbitrary processing times and a fixed number of machines.

% , in order to obtain 
% a locally consistent probability distribution on pairs of jobs.

With the same team of authors, we also obtained a polylogarithmic approximation algorithms for the more general setting of
% of scheduling with precedence constraints and communication delays on 
related machines with the objective of minimizing the weighted sum of completion times~\cite{DaviesKRTZ21}.
% This objective is minimizing $\sum_j w_j C_j$, for $w_j$ the weight of job $j$, given as input, and
% $C_j$ the completion time of job $j$.
% (referred to as $Q \mid p_j=1, c \mid  \sum_j w_j C_j$, for $C_j$ the completion time of job $j$).
% In the related machine setting, the set of machines are partitioned into speed classes, where machines in speed class $\ell$
% process job $j$ in time $p_j / s_\ell$.
% Nothing was known for this setting before our work.
This work generalizes the Sherali-Adams framework established in their previous paper, though there are new technical challenges.
For the objective of minimizing makespan, it was sufficient to determine in which $c$-length time interval a job is scheduled. 
However, for the weighted sum of completion times objective, this is not careful enough; jobs that have very small fractional completion time by the LP 
must be scheduled very early.
Additionally, as opposed to the identical machines setting, the problem now involves assigning jobs to speed classes.
We introduce more constraints into our LP and prove stronger structural insights about the Sherali-Adams solution.
Our main result is an $O(\log^4 n)$-approximation algorithm for the problem.
As a byproduct of our result, we also obtain an $O(\log^3 n)$-approximation algorithm for the problem of minimizing makespan  $\Q \mid \Prec, c \mid C_{\textrm{max}}$, which improves upon the $O(\log^5 n/\log \log n)$-approximation algorithm due to a recent work of  Maiti et al.~\cite{MRSSV}.


We consider the problem of scheduling jobs with communication delays on related machines
to minimize the weighted sum of completion times.
Let $M$ denote the number of speed classes, i.e. the number of distinct $s_i$.
While only losing a constant factor in the approximation factor, we can assume that $M =\log(s_{\text{max}}/s_{\text{min}})\leq O(\log m)$,
where $s_{\text{max}}$ and $s_{\text{min}}$ are the fastest and slowest speeds of machines.
Our main result in the related machines setting is the following theorem.
\begin{theorem}
There is a randomized $O(M^2 \cdot \log^2 n)$-approximation algorithm for 
scheduling with communication delays on related machines for minimzing the weighed sum of completion times with expected polynomial running time. 
When jobs have unit processing lengths, the approximation factor of the algorithm improves to  $O(M \cdot \log^2 n )$.
\end{theorem}
Due to the bound on $M$, the theorem implies an $O(\log^4 n)$-approximation algorithm to the general problem and an $O(\log^3 n)$-approximation algorithm when all jobs are unit-length.
We also obtain an improved approximation factor for the objective of minimizing makespan.


\begin{theorem} \label{thm:colmain}
There is a randomized $O(M \cdot \log m \cdot \log n)$-approximation algorithm for 
scheduling with communication delays on related machines for minimzing makespan
with expected polynomial running time. When jobs have unit processing lengths the approximation factor of the algorithm improves to  $O(\log m \cdot \log n)$.
\end{theorem}



